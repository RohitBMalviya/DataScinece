{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3f32f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part I\n",
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize                 # Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb80c1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "document = 'Hello Artificial Intelligence And Data Science Engineering Welcoming to the world happiness to see you all, all the best for the future management. It is the example for text analytics'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba67c831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization Example:  ['hello', 'data', 'science']\n",
      "\n",
      "Tokenization Document:  ['Hello', 'Artificial', 'Intelligence', 'And', 'Data', 'Science', 'Engineering', 'Welcoming', 'to', 'the', 'world', 'happiness', 'to', 'see', 'you', 'all', ',', 'all', 'the', 'best', 'for', 'the', 'future', 'management', '.', 'It', 'is', 'the', 'example', 'for', 'text', 'analytics']\n"
     ]
    }
   ],
   "source": [
    "print('Tokenization Example: ',word_tokenize('hello data science'))\n",
    "print()\n",
    "print('Tokenization Document: ',word_tokenize(document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2753a43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag           # Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a22a851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tagging Example:  [('hello', 'NN'), ('Data', 'NNS')]\n",
      "\n",
      "POS Tagging Document:  [('Hello', 'NNP'), ('Artificial', 'NNP'), ('Intelligence', 'NNP'), ('And', 'CC'), ('Data', 'NNP'), ('Science', 'NNP'), ('Engineering', 'NNP'), ('Welcoming', 'NNP'), ('to', 'TO'), ('the', 'DT'), ('world', 'NN'), ('happiness', 'NN'), ('to', 'TO'), ('see', 'VB'), ('you', 'PRP'), ('all', 'DT'), (',', ','), ('all', 'PDT'), ('the', 'DT'), ('best', 'JJS'), ('for', 'IN'), ('the', 'DT'), ('future', 'JJ'), ('management', 'NN'), ('.', '.'), ('It', 'PRP'), ('is', 'VBZ'), ('the', 'DT'), ('example', 'NN'), ('for', 'IN'), ('text', 'NN'), ('analytics', 'NNS')]\n"
     ]
    }
   ],
   "source": [
    "print('POS Tagging Example: ',pos_tag(['hello', 'Data']))\n",
    "print()\n",
    "words = word_tokenize(document)\n",
    "tags = pos_tag(words)\n",
    "print('POS Tagging Document: ',tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b81b00a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords          # stop word removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ce59971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop Word Removal Document:  ['Hello', 'Artificial', 'Intelligence', 'Data', 'Science', 'Engineering', 'Welcoming', 'world', 'happiness', 'see', ',', 'best', 'future', 'management', '.', 'example', 'text', 'analytics']\n"
     ]
    }
   ],
   "source": [
    "stopswords = set(stopwords.words('english'))\n",
    "# stopwordremoval = [word for word in words if word.lower() in stopswords]\n",
    "stopwordremoval = [word for word in words if word.lower() not in stopswords]\n",
    "print('Stop Word Removal Document: ',stopwordremoval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88825eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer          # for Stemming\n",
    "from nltk.stem import WordNetLemmatizer      # for lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01826d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming Example:  manag\n",
      "Stemming Example:  play\n",
      "Stemming Example:  commun\n",
      "\n",
      "Stemming Document:  ['hello', 'artifici', 'intellig', 'data', 'scienc', 'engin', 'welcom', 'world', 'happi', 'see', ',', 'best', 'futur', 'manag', '.', 'exampl', 'text', 'analyt']\n"
     ]
    }
   ],
   "source": [
    "stemming = PorterStemmer()\n",
    "\n",
    "print('Stemming Example: ',stemming.stem('Management'))\n",
    "print('Stemming Example: ',stemming.stem('playing'))\n",
    "print('Stemming Example: ',stemming.stem('communication'))\n",
    "print()\n",
    "\n",
    "stemmed_word = [stemming.stem(word) for word in stopwordremoval]\n",
    "print('Stemming Document: ',stemmed_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f33a0e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization Example :  communication\n",
      "\n",
      "Lemmatization Document:  ['Hello', 'Artificial', 'Intelligence', 'Data', 'Science', 'Engineering', 'Welcoming', 'world', 'happiness', 'see', ',', 'best', 'future', 'management', '.', 'example', 'text', 'analytics']\n"
     ]
    }
   ],
   "source": [
    "lemmatization = WordNetLemmatizer()\n",
    "print('Lemmatization Example : ',lemmatization.lemmatize('communication','v'))\n",
    "print()\n",
    "lemmatized_words = [lemmatization.lemmatize(word,pos='v') for word in stopwordremoval]\n",
    "print('Lemmatization Document: ',lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "974f91c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part II\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d7b10008",
   "metadata": {},
   "outputs": [],
   "source": [
    "document =['data science is one of the most important fields of science',\n",
    "          'this is one of the best data science courses',\n",
    "          'data scientists analyze and visualize data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f655b547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_docs = len(document)\n",
    "# n_words_set = len(words_set)\n",
    "\n",
    "# df_tf = pd.DataFrame(np.zeros((n_docs,n_words_set)),columns = words_set)\n",
    "\n",
    "# for i in range(n_docs):\n",
    "#     words = document[i].split(' ')\n",
    "#     for w in words:\n",
    "#         df_tf[w][i] = df_tf[w][i] + (1 / len(words))\n",
    "#     df_tf\n",
    "    \n",
    "# print(df_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "73b24497",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "42aa400a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.         0.18952581 0.32089509\n",
      "  0.32089509 0.24404899 0.32089509 0.48809797 0.24404899 0.48809797\n",
      "  0.         0.24404899 0.         0.        ]\n",
      " [0.         0.         0.40029393 0.40029393 0.23642005 0.\n",
      "  0.         0.30443385 0.         0.30443385 0.30443385 0.30443385\n",
      "  0.         0.30443385 0.40029393 0.        ]\n",
      " [0.4305185  0.4305185  0.         0.         0.50854232 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.4305185  0.         0.         0.4305185 ]]\n"
     ]
    }
   ],
   "source": [
    "if_idf = TfidfVectorizer()\n",
    "if_idf_document = if_idf.fit_transform(document)\n",
    "print(if_idf_document.toarray())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
